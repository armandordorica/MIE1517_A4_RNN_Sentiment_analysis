{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIR3Q5l0v1HP"
   },
   "source": [
    "# PART B - Transfer Learning\n",
    "\n",
    "For many natural language processing tasks, it is generally not a good idea to train a very large deep neural network model from scratch due to enormous compute requirements and lack of sufficient amounts of training data. Instead, you should always try to take advantage of an existing model that performs similar tasks as the one you need to solve.\n",
    "\n",
    "In this part of the assignment we will be using pretrained models to improve the performance on identifying positive and negative reviews. There are several pretrained models that are available to us, here we will use a pretrained BERT model that comes with the hugging face transformer library.\n",
    "\n",
    "Provided below is sample code to get you started. For more details please visit the hugging face tutorial on using pretrained models using PyTorch: https://huggingface.co/docs/transformers/training\n",
    "\n",
    "#### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5R-_BQyiAMoo"
   },
   "outputs": [],
   "source": [
    "# # install relevant libraries\n",
    "# !pip install -qq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "316Xs5WBv0kg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aordorica/miniforge3/envs/pytorch_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load relevant libraries\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_20pdf-tYySb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aordorica/miniforge3/envs/pytorch_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_txt = 'I want to learn how to do sentiment analysis using BERT and tokenizer.'\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=32,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    "  truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O1B4RiOOcbGY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   146,  1328,  1106,  3858,  1293,  1106,  1202, 17024,  3622,\n",
       "          1606,   139,  9637,  1942,  1105, 22559, 17260,   119,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7sZ1j9PfceFF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dZ458Q_uYzxu"
   },
   "outputs": [],
   "source": [
    "hidden_states = bert_model(input_ids=encoding['input_ids'],\n",
    "                           attention_mask=encoding['attention_mask'])[0]\n",
    "pooled_output = bert_model(input_ids=encoding['input_ids'],\n",
    "                           attention_mask=encoding['attention_mask'])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GqwUAEIzZNBO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "torch.Size([1, 32, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = bert_model.config.hidden_size\n",
    "\n",
    "print(hidden_size)\n",
    "print(hidden_states.shape)\n",
    "print(pooled_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HCaqLDaZo2l"
   },
   "source": [
    "In the sample code provided we loaded a short text sequence, tokenized it using the same tokenization that was used in the pretrained BERT model, and fed the tokenized input into the BERT model to obtain the embeddings.\n",
    "\n",
    "The model output consists of two forms of embeddings:\n",
    "- **hidden_states** are the final layer of outputs that has a shape sequence_length x embeddings, much like the hidden states of a recurrent neural network\n",
    "- **pooled_output** is the result of applying max pooling on the hidden states to effectively collapse the sequence dimenension and ensure the same output size for any given sequence before feeding into the classification stage\n",
    "\n",
    "Note that you can preprocess all of the data prior to training a classifier stage for sentiment analysis to help speed up the training process. This is no different from the process we applied in an earlier assignment using AlexNet and image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4le9bqnuc2JZ"
   },
   "source": [
    "## Part 1. Data Loading [5 pt]\n",
    "\n",
    "We will be using the same \"IMDB Movie Review Dataset\" that we used earlier. Reload the data and complete Part B of the assignment. You should be able to complete part B independently from Part A.\n",
    "\n",
    "### Part (i) [1pt EXPLORATORY] - create train/test/val dataloaders\n",
    "\n",
    "Provided below is a DataLoader for your training and test datasets so you can iterate over batches of data. Run the DataLoader to create your training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LzlpaXmteHQd"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MovieReviewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        # Convert target to a numerical value if it's a string\n",
    "        if target == 'positive':\n",
    "            target = 1\n",
    "        elif target == 'negative':\n",
    "            target = 0\n",
    "        else:\n",
    "            # Handle unexpected target value\n",
    "            raise ValueError(\"Unexpected target value\")\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',  # Replaced pad_to_max_length with padding\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>random_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.817308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.784034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.373522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.574912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.597060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  random_number\n",
       "0  One of the other reviewers has mentioned that ...  positive       0.817308\n",
       "1  A wonderful little production. <br /><br />The...  positive       0.784034\n",
       "2  I thought this was a wonderful way to spend ti...  positive       0.373522\n",
       "3  Basically there's a family where a little boy ...  negative       0.574912\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive       0.597060"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# download IMDB review data\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "# process into data and labels\n",
    "X = df['review'].values\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Assigning a random number between 0 and 1 to each row in the DataFrame\n",
    "df['random_number'] = np.random.rand(df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset based on the random number for a 60-20-20 split\n",
    "train_df = df[df['random_number'] <= 0.6]\n",
    "val_df = df[(df['random_number'] > 0.6) & (df['random_number'] <= 0.8)]\n",
    "test_df = df[df['random_number'] > 0.8]\n",
    "\n",
    "# You can now drop the 'random_number' column if it's no longer needed\n",
    "train_df = train_df.drop('random_number', axis=1)\n",
    "val_df = val_df.drop('random_number', axis=1)\n",
    "test_df = test_df.drop('random_number', axis=1)\n",
    "\n",
    "# The datasets are now split into train (60%), validation (20%), and test (20%) sets\n",
    "# train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare data loaders\n",
    "MAX_LEN = 400\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data = MovieReviewDataset(reviews=train_df['review'].to_numpy(),\n",
    "                              targets=train_df['sentiment'].to_numpy(),\n",
    "                              tokenizer=tokenizer,\n",
    "                              max_len=MAX_LEN)\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=0)  # modify num_workers as needed\n",
    "\n",
    "\n",
    "val_data = MovieReviewDataset(reviews=val_df['review'].to_numpy(),\n",
    "                              targets=val_df['sentiment'].to_numpy(),\n",
    "                              tokenizer=tokenizer,\n",
    "                              max_len=MAX_LEN)\n",
    "\n",
    "val_data_loader = DataLoader(val_data, batch_size=BATCH_SIZE, num_workers=0)  # modify num_workers as needed\n",
    "\n",
    "# Test data\n",
    "test_data = MovieReviewDataset(reviews=test_df['review'].to_numpy(),\n",
    "                               targets=test_df['sentiment'].to_numpy(),\n",
    "                               tokenizer=tokenizer,\n",
    "                               max_len=MAX_LEN)\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=0)  # modify num_workers as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0ymyMvzfdVv"
   },
   "source": [
    "### Part (ii) [1pt EXPLORATORY] - load one sample with `train_data_loader`\n",
    "\n",
    "Use the **train_data_loader** to load one sample. What are the different attributes provided with the sample and how are they used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UgdwGNtik1ZR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews: ['I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.', \"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\", 'Petter Mattei\\'s \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler\\'s play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei\\'s direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.', \"I sure would like to see a resurrection of a up dated Seahunt series with the tech they have today it would bring back the kid excitement in me.I grew up on black and white TV and Seahunt with Gunsmoke were my hero's every week.You have my vote for a comeback of a new sea hunt.We need a change of pace in TV and this would work for a world of under water adventure.Oh by the way thank you for an outlet like this to view many viewpoints about TV and the many movies.So any ole way I believe I've got what I wanna say.Would be nice to read some more plus points about sea hunt.If my rhymes would be 10 lines would you let me submit,or leave me out to be in doubt and have me to quit,If this is so then I must go so lets do it.\", \"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film. <br /><br />The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\", \"I saw this movie when I was about 12 when it came out. I recall the scariest scene was the big bird eating men dangling helplessly from parachutes right out of the air. The horror. The horror.<br /><br />As a young kid going to these cheesy B films on Saturday afternoons, I still was tired of the formula for these monster type movies that usually included the hero, a beautiful woman who might be the daughter of a professor and a happy resolution when the monster died in the end. I didn't care much for the romantic angle as a 12 year old and the predictable plots. I love them now for the unintentional humor.<br /><br />But, about a year or so later, I saw Psycho when it came out and I loved that the star, Janet Leigh, was bumped off early in the film. I sat up and took notice at that point. Since screenwriters are making up the story, make it up to be as scary as possible and not from a well-worn formula. There are no rules.\", 'So im not a big fan of Boll\\'s work but then again not many are. I enjoyed his movie Postal (maybe im the only one). Boll apparently bought the rights to use Far Cry long ago even before the game itself was even finsished. <br /><br />People who have enjoyed killing mercs and infiltrating secret research labs located on a tropical island should be warned, that this is not Far Cry... This is something Mr Boll have schemed together along with his legion of schmucks.. Feeling loneley on the set Mr Boll invites three of his countrymen to play with. These players go by the names of Til Schweiger, Udo Kier and Ralf Moeller.<br /><br />Three names that actually have made them selfs pretty big in the movie biz. So the tale goes like this, Jack Carver played by Til Schweiger (yes Carver is German all hail the bratwurst eating dudes!!) However I find that Tils acting in this movie is pretty badass.. People have complained about how he\\'s not really staying true to the whole Carver agenda but we only saw carver in a first person perspective so we don\\'t really know what he looked like when he was kicking a**.. <br /><br />However, the storyline in this film is beyond demented. We see the evil mad scientist Dr. Krieger played by Udo Kier, making Genetically-Mutated-soldiers or GMS as they are called. Performing his top-secret research on an island that reminds me of \"SPOILER\" Vancouver for some reason. Thats right no palm trees here. Instead we got some nice rich lumberjack-woods. We haven\\'t even gone FAR before I started to CRY (mehehe) I cannot go on any more.. If you wanna stay true to Bolls shenanigans then go and see this movie you will not be disappointed it delivers the true Boll experience, meaning most of it will suck.<br /><br />There are some things worth mentioning that would imply that Boll did a good work on some areas of the film such as some nice boat and fighting scenes. Until the whole cromed/albino GMS squad enters the scene and everything just makes me laugh.. The movie Far Cry reeks of scheisse (that\\'s poop for you simpletons) from a fa,r if you wanna take a wiff go ahead.. BTW Carver gets a very annoying sidekick who makes you wanna shoot him the first three minutes he\\'s on screen.', \"The cast played Shakespeare.<br /><br />Shakespeare lost.<br /><br />I appreciate that this is trying to bring Shakespeare to the masses, but why ruin something so good.<br /><br />Is it because 'The Scottish Play' is my favorite Shakespeare? I do not know. What I do know is that a certain Rev Bowdler (hence bowdlerization) tried to do something similar in the Victorian era.<br /><br />In other words, you cannot improve perfection.<br /><br />I have no more to write but as I have to write at least ten lines of text (and English composition was never my forte I will just have to keep going and say that this movie, as the saying goes, just does not cut it.\", \"Kind of drawn in by the erotic scenes, only to realize this was one of the most amateurish and unbelievable bits of film I've ever seen. Sort of like a high school film project. What was Rosanna Arquette thinking?? And what was with all those stock characters in that bizarre supposed Midwest town? Pretty hard to get involved with this one. No lessons to be learned from it, no brilliant insights, just stilted and quite ridiculous (but lots of skin, if that intrigues you) videotaped nonsense....What was with the bisexual relationship, out of nowhere, after all the heterosexual encounters. And what was with that absurd dance, with everybody playing their stereotyped roles? Give this one a pass, it's like a million other miles of bad, wasted film, money that could have been spent on starving children or Aids in Africa.....\", \"An awful film! It must have been up against some real stinkers to be nominated for the Golden Globe. They've taken the story of the first famous female Renaissance painter and mangled it beyond recognition. My complaint is not that they've taken liberties with the facts; if the story were good, that would perfectly fine. But it's simply bizarre -- by all accounts the true story of this artist would have made for a far better film, so why did they come up with this dishwater-dull script? I suppose there weren't enough naked people in the factual version. It's hurriedly capped off in the end with a summary of the artist's life -- we could have saved ourselves a couple of hours if they'd favored the rest of the film with same brevity.\", \"After the success of Die Hard and it's sequels it's no surprise really that in the 1990s, a glut of 'Die Hard on a .....' movies cashed in on the wrong guy, wrong place, wrong time concept. That is what they did with Cliffhanger, Die Hard on a mountain just in time to rescue Sly 'Stop or My Mom Will Shoot' Stallone's career.<br /><br />Cliffhanger is one big nit-pickers dream, especially to those who are expert at mountain climbing, base-jumping, aviation, facial expressions, acting skills. All in all it's full of excuses to dismiss the film as one overblown pile of junk. Stallone even managed to get out-acted by a horse! However, if you an forget all the nonsense, it's actually a very lovable and undeniably entertaining romp that delivers as plenty of thrills, and unintentionally, plenty of laughs.<br /><br />You've got to love John Lithgows sneery evilness, his tick every box band of baddies, and best of all, the permanently harassed and hapless 'turncoat' agent, Rex Linn as Travers.<br /><br />He may of been Henry in 'Portrait of a Serial Killer' but Michael Rooker is noteworthy for a cringe-worthy performance as Hal, he insists on constantly shrieking in painful disbelief at his captors 'that man never hurt anybody' And whilst he surely can't be, it really does look like Ralph Waite's Frank character is grinning as the girl plummets to her death.<br /><br />Mention too must go to former 'London's Burning' actor Craig Fairbrass as the Brit bad guy, who comes a cropper whilst using Hal as a Human Football, yes, you can't help enjoy that bit, Hal needed a good kicking.<br /><br />So forget your better judgement, who cares if 'that could never happen', lower your acting expectations, turn up the volume and enjoy! And if you're looking for Qaulen, he's the one wearing the helicopter.\", 'I had the terrible misfortune of having to view this \"b-movie\" in it\\'s entirety.<br /><br />All I have to say is--- save your time and money!!! This has got to be the worst b-movie of all time, it shouldn\\'t even be called a b-movie, more like an f-movie! Because it fails in all aspects that make a good movie: the story is not interesting at all, all of the actors are paper-thin and not at all believable, it has bad direction and the action sequences are so fake it\\'s almost funny.......almost.<br /><br />The movie is just packed full of crappy one-liners that no respectable person could find amusing in the least little bit.<br /><br />This movie is supposed to be geared towards men, but all the women in it are SO utterly unattractive, especially that old wrinkled thing that comes in towards the end. They try to appear sexy in those weird, horrible costumes and they fail miserably!!!<br /><br />Even some of the most ridiculous b-movies will still give you some laughs, but this is just too painful to watch!!', 'First of all, let\\'s get a few things straight here: a) I AM an anime fan- always has been as a matter of fact (I used to watch Speed Racer all the time in Preschool). b) I DO like several B-Movies because they\\'re hilarious. c) I like the Godzilla movies- a lot.<br /><br />Moving on, when the movie first comes on, it seems like it\\'s going to be your usual B-movie, down to the crappy FX, but all a sudden- BOOM! the anime comes on! This is when the movie goes WWWAAAAAYYYYY downhill.<br /><br />The animation is VERY bad & cheap, even worse than what I remember from SPEED RACER, for crissakes! In fact, it\\'s so cheap, one of the few scenes from the movie I \"vividly\" remember is when a bunch of kids run out of a school... & it\\'s the same kids over & over again! The FX are terrible, too; the dinosaurs look worse than Godzilla. In addition, the transition to live action to animation is unorganized, the dialogue & voices(especially the English dub that I viewed) was horrid & I was begging my dad to take the tape out of the DVD/ VHS player; The only thing that kept me surviving was cracking out jokes & comments like the robots & Joel/Mike on MST3K (you pick the season). Honestly, this is the only way to barely enjoy this movie & survive it at the same time.<br /><br />Heck, I\\'m planning to show this to another fellow otaku pal of mine on Halloween for a B-Movie night. Because it\\'s stupid, pretty painful to watch & unintentionally hilarious at the same time, I\\'m giving this movie a 3/10, an improvement from the 0.5/10 I was originally going to give it.<br /><br />(According to my grading scale: 3/10 means Pretty much both boring & bad. As fun as counting to three unless you find a way to make fun of it, then it will become as fun as counting to 15.)', \"This was the worst movie I saw at WorldFest and it also received the least amount of applause afterwards! I can only think it is receiving such recognition based on the amount of known actors in the film. It's great to see J.Beals but she's only in the movie for a few minutes. M.Parker is a much better actress than the part allowed for. The rest of the acting is hard to judge because the movie is so ridiculous and predictable. The main character is totally unsympathetic and therefore a bore to watch. There is no real emotional depth to the story. A movie revolving about an actor who can't get work doesn't feel very original to me. Nor does the development of the cop. It feels like one of many straight-to-video movies I saw back in the 90s ... And not even a good one in those standards.<br /><br />\", \"The Karen Carpenter Story shows a little more about singer Karen Carpenter's complex life. Though it fails in giving accurate facts, and details.<br /><br />Cynthia Gibb (portrays Karen) was not a fine election. She is a good actress , but plays a very naive and sort of dumb Karen Carpenter. I think that the role needed a stronger character. Someone with a stronger personality.<br /><br />Louise Fletcher role as Agnes Carpenter is terrific, she does a great job as Karen's mother.<br /><br />It has great songs, which could have been included in a soundtrack album. Unfortunately they weren't, though this movie was on the top of the ratings in USA and other several countries\", 'Taut and organically gripping, Edward Dmytryk\\'s Crossfire is a distinctive suspense thriller, an unlikely \"message\" movie using the look and devices of the noir cycle.<br /><br />Bivouacked in Washington, DC, a company of soldiers cope with their restlessness by hanging out in bars. Three of them end up at a stranger\\'s apartment where Robert Ryan, drunk and belligerent, beats their host (Sam Levene) to death because he happens to be Jewish. Police detective Robert Young investigates with the help of Robert Mitchum, who\\'s assigned to Ryan\\'s outfit. Suspicion falls on the second of the three (George Cooper), who has vanished. Ryan slays the third buddy (Steve Brodie) to insure his silence before Young closes in.<br /><br />Abetted by a superior script by John Paxton, Dmytryk draws precise performances from his three starring Bobs. Ryan, naturally, does his prototypical Angry White Male (and to the hilt), while Mitchum underplays with his characteristic alert nonchalance (his role, however, is not central); Young may never have been better. Gloria Grahame gives her first fully-fledged rendition of the smart-mouthed, vulnerable tramp, and, as a sad sack who\\'s leeched into her life, Paul Kelly haunts us in a small, peripheral role that he makes memorable.<br /><br />The politically engaged Dmytryk perhaps inevitably succumbs to sermonizing, but it\\'s pretty much confined to Young\\'s reminiscence of how his Irish grandfather died at the hands of bigots a century earlier (thus, incidentally, stretching chronology to the limit). At least there\\'s no attempt to render an explanation, however glib, of why Ryan hates Jews (and hillbillies and...).<br /><br />Curiously, Crossfire survives even the major change wrought upon it -- the novel it\\'s based on (Richard Brooks\\' The Brick Foxhole) dealt with a gay-bashing murder. But homosexuality in 1947 was still Beyond The Pale. News of the Holocaust had, however, begun to emerge from the ashes of Europe, so Hollywood felt emboldened to register its protest against anti-Semitism (the studios always quaked at the prospect of offending any potential ticket buyer).<br /><br />But while the change from homophobia to anti-Semitism works in general, the specifics don\\'t fit so smoothly. The victim\\'s chatting up a lonesome, drunk young soldier then inviting him back home looks odd, even though (or especially since) there\\'s a girlfriend in tow. It raises the question whether this scenario was retained inadvertently or left in as a discreet tip-off to the original engine generating Ryan\\'s murderous rage.']\n",
      "Input IDs: tensor([[  101,   146,  1354,  ...,     0,     0,     0],\n",
      "        [  101, 11568,  2716,  ...,     0,     0,     0],\n",
      "        [  101, 25993,  2083,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1188,  1108,  ...,     0,     0,     0],\n",
      "        [  101,  1109,  7677,  ...,     0,     0,     0],\n",
      "        [  101, 27046,  1204,  ...,  1830,   117,   102]])\n",
      "Attention Masks: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "Targets: tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load a single batch from the DataLoader\n",
    "for batch in train_data_loader:\n",
    "    # We'll just look at the first batch and then break out of the loop\n",
    "    break\n",
    "\n",
    "# The reviews in text form\n",
    "reviews = batch['review_text']\n",
    "print(\"Reviews:\", reviews)\n",
    "\n",
    "# The input IDs from the tokenizer\n",
    "input_ids = batch['input_ids']\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "# The attention masks from the tokenizer\n",
    "attention_masks = batch['attention_mask']\n",
    "print(\"Attention Masks:\", attention_masks)\n",
    "\n",
    "# The target labels (sentiments)\n",
    "targets = batch['targets']\n",
    "print(\"Targets:\", targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXQt2HiGfh4p"
   },
   "source": [
    "### Part (iii) [1pt EXPLORATORY]\n",
    "\n",
    "Determine the range of values for the tokens in the training data. How are the tokens obtained?\n",
    "\n",
    "Hint: You can apply your intuition here, or do some additional research to find how the \"bert-base-cased\" tokenization is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6cVEH5YhkYUQ"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Im2KNoOPdYHU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPROVIDE YOUR ANSWER BELOW\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CYKglzbfTmw"
   },
   "source": [
    "### Part (iv) [1pt EXPLORATORY]\n",
    "\n",
    "Generate histograms of all the token values in the training data. Repeat for the validation and test data. What are the top 5 occuring tokens in the training_dataset? What do these tokens represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HTZdkR7ukhRg"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fS1c0a6wddHl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPROVIDE YOUR ANSWER BELOW\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ElnJDdgcaiy"
   },
   "source": [
    "### Part (v) [1pt EXPLORATORY]\n",
    "\n",
    "Select a single sample from your training DataLoader and feed it through the **bert_model** to obtain the hidden_states and pooled_output. Briefly describe what each tensor dimension represents and what affects the size of each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1HOknY9Ncai9"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "34OnAb9EdjtW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPROVIDE YOUR ANSWER BELOW\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDbQ7nRBek5S"
   },
   "source": [
    "## Part 2. Model Architecture [2 pt]\n",
    "\n",
    "### Part (i) [1pt MODEL]\n",
    "\n",
    "Prepare a review classifier model that builds on the pooled output from the Bert model to identify positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kAISbN5VexqS"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSentimentClassifierPooled\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_classes):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m(SentimentClassifierPooled, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class SentimentClassifierPooled(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifierPooled, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "    # TO BE COMPLETED\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "    # TO BE COMPLETED\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLFzooLUgn3c"
   },
   "source": [
    "### Part (ii) [1pt MODEL]\n",
    "\n",
    "Construct the architecture for a review classifier model that uses the last hidden output from the Bert model to identify positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q34oWGhoe38H"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifierLast(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifierLast, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "    # TO BE COMPLETED\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "\n",
    "    # TO BE COMPLETED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqkKp1CgiYZb"
   },
   "source": [
    "## Part 3. Training [3 pt]\n",
    "\n",
    "### Part (i) [1pt MODEL]\n",
    "\n",
    "Complete the `get_accuracy` function, which will compute the\n",
    "accuracy (rate) of your model across a dataset (e.g. validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC89f2i7iYZl"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, data):\n",
    "    \"\"\" Compute the accuracy of the `model` across a dataset `data`\n",
    "\n",
    "    Example usage:\n",
    "\n",
    "    >>> model = MyRNN() # to be defined\n",
    "    >>> get_accuracy(model, valid_loader) # the variable `valid_loader` is from above\n",
    "    \"\"\"\n",
    "\n",
    "    # TO BE COMPLETED\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruaRg5JCiYZm"
   },
   "source": [
    "### Part (ii) [1pt MODEL]\n",
    "\n",
    "Write a function **train_model** to train your model. Plot the training curve of your final model.\n",
    "Your training curve should have the training/validation loss and\n",
    "accuracy plotted periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuqypAPxiYZm"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVvzpQRWiYZm"
   },
   "source": [
    "### Part (iii) [1pt MODEL]\n",
    "\n",
    "Choose at least 4 hyperparameters to tune. Explain how you tuned the hyperparameters. You don't need to include your training curve for every model you trained.\n",
    "Instead, explain what hyperparemters you tuned, what the best validation accuracy was,\n",
    "and the reasoning behind the hyperparameter decisions you made.\n",
    "\n",
    "For this assignment, you should tune more than just your learning rate and epoch.\n",
    "Choose at least 2 hyperparameters that are unrelated to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeCZtA9iiYZm"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9o-7udJmahe"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecKaJIpQe9UV"
   },
   "source": [
    "## Part 4. Evaluation [10 pt]\n",
    "\n",
    "### Part (i) [3pt RESULT]\n",
    "\n",
    "Report the final test accuracy of your best BERT-based model. Then summarize in a pandas dataframe the accuracy obtained on the training, validation, and test data of your best models from Part A and B.\n",
    "\n",
    "How does the BERT model compare to the approach in part A using only LSTM? Are the results what you expected? Explain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoweZb8i2cRb"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RIZ3JWzm2a_"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybdyEKx5Ww4F"
   },
   "source": [
    "### Part (ii) [2pt RESULT]\n",
    "\n",
    "Report the false positive rate and false negative rate of your model across the test set. Then summarize in a pandas dataframe the false postive and false negative rate of your model obtained on the training, validation, and test data of your best models from Part A and B.\n",
    "\n",
    "How does the BERT model compare to the approach in part A using only LSTM? Are the results what you expected? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuxLCHhFWw4F"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIu8lB9jm3X_"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "646WjsAwWw4F"
   },
   "source": [
    "### Part (iii) [3pt DISCUSSION]\n",
    "Examine some of the misclassified reviews from you best BERT and LSTM models to better identify the differences in the models. Try to provide some justification for any differences in the misclassifications observed in the models.\n",
    "\n",
    "Is there any part of the review that you could modify to make the classifications correct? Try to make small changes to the review to see if you can make the model make the correct classification while keeping the review as close to the original as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OvvnW8OWw4G"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TWkK5Sgm4YM"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4FlSj_Mn2yY"
   },
   "source": [
    "### Part (iv) [2pt DISCUSSION]\n",
    "Find 5 samples of positive and negative reviews on IMDB that were posted recently and evaluate them with your best BERT and LSTM models from parts A and B. How well do they perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3DCyMW_n05f"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kLt9a5Xm5cO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARDyU7N2mWFh"
   },
   "source": [
    "# PART C (Optional) - Bonus Challenge!\n",
    "\n",
    "This is an optional exercise for those that finish the assignment early and would like to take on a challenging task.\n",
    "\n",
    "In part A we constructed and trained an LSTM model to identify the sentiment in movie reviews. In Part B we used the embeddings of a BERT model pretrained on a large corpus of text to demonstrate how transfer learning can be used to improve our movie sentiment model. The BERT model is one of many language models that we could have used to implement transfer learning.\n",
    "\n",
    "For this bonus challenge you are asked to implement a generative character-level LSTM model to produce IMDB movie reviews. Once the model is sufficiently trained you can then use its hidden states as the embedding for training a movie sentiment model. Construct your new movie sentiment analysis model and compare the performance against the model from part A and B.\n",
    "\n",
    "There are many variants of a generative LSTM model that you can consider. As a starting point you can use the generative LSTM sample code provided in the lecture notes. Specifically, the one used to generate Shakeaspeare. More advanced versions of a generative LSTM can be found in the Universal Language Model Fine-turing for Text Classification (ULMfit) paper (https://arxiv.org/abs/1801.06146).\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Create a generative character-level LSTM model trained to create IMDB reviews\n",
    "2. Create a classifier using the embeddings from the generative LSTM model (from step 1) to identify positive and negative reviews.\n",
    "3. Compare the performance of the model with the results in parts A and B of the assignment.\n",
    "4. Upgrade the generative LSTM model using the techniques listed in the ULMfit paper (e.g., bi-directional LSTM, pretraining with wikipedia text and fine-tuning on IMDBT reviews, etc.).\n",
    "\n",
    "Bonus marks will be provided based on the number of steps completed. Summarize below your results and anything intersting you learned from the steps that you completed. Bonus marks cannot be accumulated beyond a maximum assignment grade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eUKPfCumdpW"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryS5uR0NB8pE"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYwI4RmFS2RB"
   },
   "source": [
    "### Saving to HTML\n",
    "Detailed instructions for saving to HTML can be found <a href=\"https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab/64487858#64487858\">here</a>. Provided below are a summary of the instructions:\n",
    "\n",
    "(1) download your ipynb file by clicking on File->Download.ipynb\n",
    "\n",
    "(2) reupload your file to the temporary Google Colab storage (you can access the temporary storage from the tab to the left)\n",
    "\n",
    "(3) run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TrsqdNgS5ex",
    "outputId": "9e08e659-76b4-4d8f-a5f3-c49a3cbe2096"
   },
   "outputs": [],
   "source": [
    "#!pip install nbconvert\n",
    "\n",
    "%%shell\n",
    "jupyter nbconvert --to html /content/A4.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuXhlFlPTY7F"
   },
   "source": [
    "(4) the html file will be available for download in the temporary Google Colab storage\n",
    "\n",
    "(5) review the html file and make sure all the results are visible before submitting your assignment to Quercus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oo8xLbm4mggI"
   },
   "source": [
    "# Assignment Grading Rubric\n",
    "The grading of the assignment will be based on the following categories:\n",
    "\n",
    "(1) **10 Pt - EXPLORATORY QUESTIONS** These are basic questions that in most cases can be answered without requiring a fully working and trained neural network model. For example, data loading, processing and visualization, summary statistics, data exploration, model and training setup, etc.\n",
    "\n",
    "(2) **10 Pt - MODEL** Student has successfully implemented all the required neural network models and has demonstrated successful training of the model without any errors.\n",
    "\n",
    "(3) **10 Pt - RESULT** Students are evaluated based on the results achieved in comparison to the expected results of the assignment.\n",
    "\n",
    "(4) **10 Pt - DISCUSSION QUESTIONS** Student demonstrated understanding beyond the basic exploratory questions, can answer some of the more challenging questions, and provide arguments for their model selection decisions.\n",
    "\n",
    "(5) **10 Pt - COMMUNICATION** Student has provided a quality submission that is easy to read without too many unnecessary output statements that distract the reading of the document. The code has been well commented and all the answers are communicated clearly and concisely.\n",
    "\n",
    "(6) **10 Pt - BONUS** Student has completed the assignment and has taken on the challenging bonus tasks listed in PART C. The student has demonstrated a good understanding of all aspects of the assignment and has exceeded expectations for the assignment.\n",
    "\n",
    "\n",
    "\n",
    "**TOTAL GRADE = _____ of 50 Pts**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
