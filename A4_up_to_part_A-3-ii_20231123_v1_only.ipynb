{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7HnqP6_a904"
   },
   "source": [
    "### A4_up_to_part_A-3-ii_20231123_v1_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Iterate over training DataLoader\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 20\u001b[0m     reviews, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreviews\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Load the tensors\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "train_reviews_tensor = torch.load('train_reviews_tensor.pt')\n",
    "val_reviews_tensor = torch.load('val_reviews_tensor.pt')\n",
    "train_labels_tensor = torch.load('train_labels_tensor.pt')\n",
    "val_labels_tensor = torch.load('val_labels_tensor.pt')\n",
    "\n",
    "# Recreate the datasets\n",
    "train_dataset = TensorDataset(train_reviews_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_reviews_tensor, val_labels_tensor)\n",
    "\n",
    "# Recreate the DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over training DataLoader\n",
    "for i, batch in enumerate(train_loader):\n",
    "    reviews, labels = batch  # Unpack the tuple directly\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(f\"Review batch shape: {reviews.shape}\")\n",
    "    print(f\"Label batch shape: {labels.shape}\")\n",
    "    # Add a break to stop after the first batch for demonstration purposes\n",
    "    if i == 0: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # RNN Layer (choose between nn.RNN, nn.LSTM, or nn.GRU)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                           dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
    "\n",
    "        # Activation function (e.g., sigmoid for binary classification)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Embedding and RNN\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Max pooling and average pooling\n",
    "        out_max = torch.max(out, dim=1)[0]\n",
    "        out_avg = torch.mean(out, dim=1)\n",
    "        out = torch.cat([out_max, out_avg], dim=1)\n",
    "\n",
    "        # Dropout and fully connected layer\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Sigmoid function\n",
    "        sig_out = self.sigmoid(out)\n",
    "\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        # Check for MPS availability and use it if available\n",
    "        if torch.backends.mps.is_available():\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(\"mps\"),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(\"mps\"))\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "# Example: Instantiate the model with specific parameters\n",
    "vocab_size = 1000  # Size of vocabulary obtained from the training data\n",
    "output_size = 1    # Binary classification (Positive/Negative)\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "# Optionally, move the model to MPS if available\n",
    "if torch.backends.mps.is_available():\n",
    "    model.to(\"mps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKIYPl_Ba90_"
   },
   "source": [
    "## Part 3. Training [3 pt]\n",
    "\n",
    "### Part (i) [1pt MODEL] - get_accuracy function\n",
    "\n",
    "Complete the `get_accuracy` function, which will compute the\n",
    "accuracy (rate) of your model across a dataset (e.g. validation set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvNfhGD6a91A"
   },
   "source": [
    "```python\n",
    "def get_accuracy(model, data):\n",
    "    \"\"\" Compute the accuracy of the `model` across a dataset `data`\n",
    "\n",
    "    Example usage:\n",
    "\n",
    "    >>> model = MyRNN() # to be defined\n",
    "    >>> get_accuracy(model, valid_loader) # the variable `valid_loader` is from above\n",
    "    \"\"\"\n",
    "\n",
    "    # TO BE COMPLETED\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def log_training_results(log_file, run_details):\n",
    "    \"\"\"\n",
    "    Logs the details of a training run to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    log_file (str): The file path for the log file.\n",
    "    run_details (dict): A dictionary containing details of the training run.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(log_file)\n",
    "    with open(log_file, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=run_details.keys())\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write headers if file does not exist\n",
    "\n",
    "        writer.writerow(run_details)\n",
    "\n",
    "\n",
    "def plot_training_curves(log_file, train_loss='train_loss', val_loss='validation_loss', train_accuracy='train_accuracy', val_accuracy='validation_accuracy'):\n",
    "    data = pd.read_csv(log_file)\n",
    "\n",
    "    # Convert string representations of lists into actual lists\n",
    "    data[train_loss] = data[train_loss].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    data[val_loss] = data[val_loss].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    data[train_accuracy] = data[train_accuracy].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    data[val_accuracy] = data[val_accuracy].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Assuming the lists for each metric contain one value per epoch\n",
    "    epochs = range(1, len(data['train_loss'].iloc[0]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, data[train_loss].iloc[0], label='Training Loss')\n",
    "    plt.plot(epochs, data[val_loss].iloc[0], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, data[train_accuracy].iloc[0], label='Training Accuracy')\n",
    "    plt.plot(epochs, data[val_accuracy].iloc[0], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def get_accuracy(model, data):\n",
    "    \"\"\" Compute the accuracy of the `model` across a dataset `data` \"\"\"\n",
    "    # Ensure model is in evaluation mode, which turns off dropout\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to track total and correct predictions\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Disable gradient calculations for efficiency\n",
    "    with torch.no_grad():\n",
    "        for batch in data:\n",
    "            # Get input data and labels\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Move data to the same device as the model\n",
    "            if torch.backends.mps.is_available():\n",
    "                inputs, labels = inputs.to(\"mps\"), labels.to(\"mps\")\n",
    "            elif torch.cuda.is_available():\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs, _ = model(inputs, model.init_hidden(inputs.size(0)))\n",
    "\n",
    "            # Convert output probabilities to predicted class (0 or 1)\n",
    "            predicted = outputs.round()  # Assuming a sigmoid activation at the output\n",
    "\n",
    "            # Count total and correct predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze() == labels).sum().item()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlxlcAC1a91C"
   },
   "source": [
    "### Part (ii) [1pt MODEL] -train model v1\n",
    "\n",
    "Train your model. Plot the training curve of your final model.\n",
    "Your training curve should have the training/validation loss and\n",
    "accuracy plotted periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CVtf7CJCa91D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x128db4190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100/1874, Current Batch Loss: 0.594249\n",
      "Epoch 1, Batch 200/1874, Current Batch Loss: 0.494879\n",
      "Epoch 1, Batch 300/1874, Current Batch Loss: 0.578292\n",
      "Epoch 1, Batch 400/1874, Current Batch Loss: 0.353466\n",
      "Epoch 1, Batch 500/1874, Current Batch Loss: 0.448770\n",
      "Epoch 1, Batch 600/1874, Current Batch Loss: 0.391535\n",
      "Epoch 1, Batch 700/1874, Current Batch Loss: 0.290971\n",
      "Epoch 1, Batch 800/1874, Current Batch Loss: 0.637351\n",
      "Epoch 1, Batch 900/1874, Current Batch Loss: 0.340215\n",
      "Epoch 1, Batch 1000/1874, Current Batch Loss: 0.255110\n",
      "Epoch 1, Batch 1100/1874, Current Batch Loss: 0.186297\n",
      "Epoch 1, Batch 1200/1874, Current Batch Loss: 0.682827\n",
      "Epoch 1, Batch 1300/1874, Current Batch Loss: 0.277466\n",
      "Epoch 1, Batch 1400/1874, Current Batch Loss: 0.418290\n",
      "Epoch 1, Batch 1500/1874, Current Batch Loss: 0.296807\n",
      "Epoch 1, Batch 1600/1874, Current Batch Loss: 0.270947\n",
      "Epoch 1, Batch 1700/1874, Current Batch Loss: 0.285718\n",
      "Epoch 1, Batch 1800/1874, Current Batch Loss: 0.521217\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m         output, _ \u001b[38;5;241m=\u001b[39m model(inputs, model\u001b[38;5;241m.\u001b[39minit_hidden(inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m     49\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 50\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Calculate average losses\u001b[39;00m\n\u001b[1;32m     53\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer (e.g., Adam, SGD, etc.)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# To store metrics\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        if torch.backends.mps.is_available():\n",
    "            inputs, labels = inputs.to(\"mps\"), labels.to(\"mps\")\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(inputs, model.init_hidden(inputs.size(0)))\n",
    "        # print(\"Output shape:\", output.shape)\n",
    "        # print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "        loss = criterion(output.squeeze(1), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Print status update every 100 batches\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i+1}/{len(train_loader)}, Current Batch Loss: {loss.item():.6f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = get_accuracy(model, val_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            if torch.backends.mps.is_available():\n",
    "                inputs, labels = inputs.to(\"mps\"), labels.to(\"mps\")\n",
    "            output, _ = model(inputs, model.init_hidden(inputs.size(0)))\n",
    "            loss = criterion(output.squeeze(1), labels.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    # Store accuracy\n",
    "    train_accuracies.append(get_accuracy(model, train_loader))\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch+1} Summary')\n",
    "    print(f'\\tTraining Loss: {train_losses[-1]:.6f} \\tTraining Accuracy: {train_accuracies[-1]:.6f}')\n",
    "    print(f'\\tValidation Loss: {val_losses[-1]:.6f} \\tValidation Accuracy: {val_accuracies[-1]:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.3367482218901327,\n",
       "  0.2909799061717592,\n",
       "  0.23552299540507812,\n",
       "  0.18485546735805514,\n",
       "  0.14560836749036984],\n",
       " [0.3883242250035444,\n",
       "  0.4027453606897754,\n",
       "  0.4354477087882432,\n",
       "  0.51256415566379,\n",
       "  0.5525463192235007])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8931325743665075,\n",
       " 0.9239141321403532,\n",
       " 0.945147397589557,\n",
       " 0.9551297032016827,\n",
       " 0.9710880379260842]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_details = {\n",
    "    'epoch': epoch,\n",
    "    'train_loss': train_losses,\n",
    "    'validation_loss': val_losses,\n",
    "    'train_accuracy': train_accuracies,\n",
    "    'validation_accuracy': val_accuracies,\n",
    "    'learning_rate': 0.001,  # Example hyperparameter\n",
    "    # Add other hyperparameters and metrics as needed\n",
    "}\n",
    "\n",
    "log_training_results('training_log_partA_3_ii_v1.csv', run_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.read_csv('training_log_partA_3_ii_v1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves('training_log_partA_3_ii_v1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script A4_up_to_part_A-3-ii_20231123.ipynb\n",
    "\n",
    "import nbformat\n",
    "\n",
    "def save_notebook_as_py(notebook_path, output_path):\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                f.write(cell.source + '\\n\\n')\n",
    "\n",
    "# Usage\n",
    "save_notebook_as_py('A4_up_to_part_A-3-ii_20231123_v1_only.ipynb', 'A4_up_to_part_A-3-ii_20231123_v1_only.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
